---
icon: form
date: 2025-09-07
category: 课程-大数据决策分析与决策
tag:
  - 课程
---
# 课程-大数据决策分析与决策
> 2025年9月7日 第一节课  
>  大数据分析和决策  
>  助教：   
>  老师: 陈涛
>   
> 本课能学到
> - 了解金融大数据的基本概念和相关理论知识    
> - 1、金融大数据知识简介      
> - 2、金融大数据理论基础
> - 3、python基础
> - 4、金融场景下的python
> - 5、python金融分析
> - 6、深度学习大数据分析
> - 7、经典案例分析和实践
> 
> 
> 考核要求： 不考试  <br>
> 考勤、课堂表现(讨论和发言) 10%   <br>
> 作业 20    <br>
> 课程报告 70%
>  <br>

## 第一次课
2025年9月7日   
### 大数据背景简介  

大数据特点：   
1、体量巨大。2025年，全球年新增数据量达到175ZB       
2、数据类型。多样化和非机构化   
3、处理速度快。从复杂数据中获取高价值信息。   
4、价值密度低。   

计算方式：         
1、云计算  
2、分布式系统（核心map和reduce）   
![img_1.png](assets/big_map_reduce.png)   

3、数据挖掘   
借助算法从海量信息中挖掘有价值信息活动   
![img.png](assets/filter.jpg)   

大数据挑战:   
1、网络架构    

2、数据中心     
为啥大数据在贵州等中西部地区？       
地皮便宜、电费便宜      
数据丢失   
数据安全  

3、运维的挑战     

### 金融大数据简介 
金融行业是典型的数据驱动行业，每天产生大量数据，包括交易、报价、业绩报告等各种指标数据     

![img_1.png](assets/finance_1.png)

金融数据产生主题： 人、物、机器

![img_1.png](assets/suppl_module.png)


### 银行大数据应用场景
#### 银行大数据核心应用场景及具体描述
1. **信贷风控：筑牢风险防线**  
   基于客户多维度数据（如征信记录、交易流水、资产负债情况、行为偏好等）构建智能风控模型，实现贷前精准画像——快速识别高风险客户，避免“带病授信”；
贷中动态监测——实时追踪客户资金流向与还款能力变化，预警逾期风险；贷后高效处置——通过数据关联分析定位失联客户、评估资产处置价值，大幅降低不良贷款率，
提升信贷业务安全性。

2. **精准营销：提升客户价值**  
   整合客户基础信息（年龄、职业、地域）、交易数据（消费类型、频次、额度）、互动行为（APP点击、咨询记录）等数据，搭建客户分层体系与需求标签库。
针对不同客群推送定制化服务：如为高频消费客户推荐信用卡分期优惠，为高净值客户匹配财富管理产品，为小微企业主推送经营性贷款方案，实现“千人千策”的营销触达，
提升转化率与客户黏性。

3. **供应链金融：激活产业链活力**  
   打通核心企业、上下游中小企业与银行的数据链路，依托核心企业的信用背书与供应链交易数据（如订单合同、物流信息、应收账款凭证等），为链条上的中小企业
提供无抵押、高效率的融资服务。例如，基于真实的应收账款数据为上游供应商提供“保理融资”，基于订单数据为下游经销商提供“订单贷”，解决中小企业融资难、
融资慢的痛点，同时降低银行信贷风险，助力产业链稳定运转。

4. **运营优化：降本增效提体验**  
   通过大数据分析优化银行内部运营与客户服务流程：在网点运营上，分析客户到访高峰时段与业务办理类型，动态调整窗口数量与人员排班，减少客户等待时间；
在线上服务上，基于客户咨询热点数据优化APP功能布局、完善智能客服知识库，提升自助服务效率；在成本控制上，分析各业务线的资金占用、人力投入与收益情况，
优化资源配置，砍掉低效业务环节，实现“降本、增效、提质”的运营目标。

5**保险、证券**


### 重点
业务代码翻译渐渐被AI替代，未来会越来越难受   
电商跨界金融给传统金融造成交大冲击（信用卡），因此，以银行为代表金融机构接到电商寻求新出路（一方面搭建自己电商、一方面和电商合作）。    

## 第二次课 
2025年9月14日   
大数据分析是指适当的统计分析方法对采集的大数据进行分析，并将这些数据加以汇总、理解和消化，提取有用的信息和形成结论，以求最大化的开发数据的功能和发挥
数据的作用。    

### 大数据分析方法类型
1、描述性分析  
描述性分析是指对数据进行描述，如数据的基本统计信息、数据的分布、数据的关系、数据的分布等。   
2、预测性分析    
预测性分析是指利用历史数据和统计模型，对未来事件或结果进行预测。     
3、规范性分析    
正规性分析是指对数据进行正则化，将数据进行转换，使其符合正态分布，从而提高数据的处理效率。     
4、关联规则分析    
关联规则分析是指对数据进行关联，找出数据之间的关联关系，从而找出数据之间的 dependencies。   
5、聚类分析    
聚类分析是指对数据进行聚类(分到未知类别)，将数据进行分类，从而找出数据之间的 dependencies。  
6、分类分析    
分类分析是指对数据进行分类(分到已知类别)，将数据进行分类，从而找出数据之间的 dependencies。  
7、时间序列分析       
时间序列分析是指对时间序列数据进行分析，如数据的趋势、季节性、周期性等。   
8、数据挖掘     
数据挖掘是指从大量数据中提取隐藏的模式、关联规则、趋势和异常值等信息的过程。它通常用于发现数据中的隐藏结构和知识，帮助组织做出更明智的决策。   
数据挖掘技术包括关联规则挖掘、聚类分析、分类分析、决策树挖掘、神经网络挖掘等。   

依据探索自然过程，可以划分为定性分析和定量分析，定型分析侧重于模物理模型的建立和数据意义的阐述；定量分析为信息研究提供数据依据，侧重于数学模型
的建立和求解。二者相互补充。   

### 大数据分析方法的步骤    
1、数据的获取存贮   
2、数据信息抽取和无用信息清洗    
3、数据整合和表述   
4、数据模型建立和结果分析   
5、结果阐述    

### 大数据预处理

大数据预处理是数据分析和机器学习流程中至关重要的一步。它的目标是将原始、杂乱、不完整或含有噪声的数据，转化为**干净、一致、适合分析或建模的格式**。如果跳过这一步，后续的分析结果可能会严重失真。

> 💡 **核心原则**：  
> **“垃圾进，垃圾出”（Garbage In, Garbage Out）**  
> 再强大的模型也救不了脏数据。

---

#### 一、大数据预处理的主要步骤

##### 1. 数据清洗（Data Cleaning）

处理数据中的“脏”部分。

- **处理缺失值**：
   - 删除含有缺失值的记录（适用于缺失少的情况）
   - 填补缺失值：用均值、中位数、众数、插值法，或机器学习预测填补
- **处理异常值（Outliers）**：
   - 使用箱线图、Z-score、IQR 等方法识别并处理
   - 可选择删除、修正或保留（取决于业务背景）
- **去重**：
   - 删除重复的记录（如用户多次提交相同信息）

---

##### 2. 数据集成（Data Integration）

将来自多个数据源的数据整合在一起。

- 合并数据库、日志文件、API 数据等
- 处理**模式冲突**（如字段名不同）、**单位不一致**（如“公斤” vs “磅”）
- 避免冗余和矛盾信息

> ⚠️ **挑战**：大数据环境下，数据来源多、格式异构（结构化、半结构化、非结构化）

---

##### 3. 数据变换（Data Transformation）

将数据转换为更适合分析的形式。

- **归一化（Normalization）**：
   - 将数值缩放到 `[0,1]` 或 `[-1,1]` 区间，常用在神经网络等模型中
   - 公式：`x' = (x - min) / (max - min)`
- **标准化（Standardization）**：
   - 转换为均值为0、标准差为1的分布
   - 公式：`x' = (x - μ) / σ`
- **离散化（Discretization）**：
   - 将连续变量转为分类变量，如年龄分为“青年、中年、老年”
- **属性构造（Feature Engineering）**：
   - 创建新特征，如从“出生日期”计算“年龄”

---

##### 4. 数据规约（Data Reduction）

在不损失关键信息的前提下，减少数据量，提高处理效率。

- **维度规约**：
   - 主成分分析（PCA）、特征选择等，降低特征数量
- **数量规约**：
   - 聚合、抽样（如随机抽样、分层抽样）、数据压缩
- **数值规约**：
   - 用参数模型（如回归）或非参数方法（如直方图）近似表示数据

---

##### 5. 数据编码（Data Encoding）

将非数值数据转换为数值形式，便于算法处理。

- **标签编码（Label Encoding）**：
   - 如：男 → 0，女 → 1
- **独热编码（One-Hot Encoding）**：
   - 将类别变量转为多个二进制列（避免引入错误的顺序关系）
   - 示例：
      - 红 → `[1, 0, 0]`
      - 绿 → `[0, 1, 0]`
      - 蓝 → `[0, 0, 1]`

---

#### 二、大数据预处理的挑战

| 挑战 | 说明 |
|------|------|
| **数据量大（Volume）** | 处理TB/PB级数据，传统工具无法胜任 |
| **数据种类多（Variety）** | 结构化（表格）、半结构化（JSON）、非结构化（文本、图像）混合 |
| **速度快（Velocity）** | 实时流数据需要在线预处理 |
| **质量差（Veracity）** | 数据噪声多、缺失严重、格式混乱 |
| **工具选择** | 需使用分布式框架（如Spark、Flink） |

---

#### 三、常用工具与技术

| 工具/平台 | 用途 |
|----------|------|
| `Python`（Pandas, NumPy, Scikit-learn） | 小到中等规模数据预处理 |
| `Apache Spark`（PySpark/Scala） | 分布式大数据清洗与转换 |
| `Hadoop + Hive` | 批量数据清洗与查询 |
| `Apache Flink` | 实时流数据预处理 |
| `OpenRefine` | 交互式数据清洗工具 |
| `SQL` | 常用于过滤、聚合、去重等操作 |

---

#### 四、一个简单示例（Python + Pandas）

```python
import pandas as pd
import numpy as np

# 读取数据
df = pd.read_csv("data.csv")

# 1. 处理缺失值
df.fillna(df.mean(numeric_only=True), inplace=True)  # 数值列用均值填充
df.dropna(subset=['name'], inplace=True)            # 关键字段为空则删除

# 2. 去重
df.drop_duplicates(inplace=True)

# 3. 处理异常值（以收入为例）
Q1 = df['income'].quantile(0.25)
Q3 = df['income'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df = df[(df['income'] >= lower_bound) & (df['income'] <= upper_bound)]

# 4. 数据标准化
df['income_scaled'] = (df['income'] - df['income'].mean()) / df['income'].std()

# 5. 类别编码
df['gender'] = df['gender'].map({'男': 0, '女': 1})

# 输出清洗后数据
df.to_csv("cleaned_data.csv", index=False)
```


## 第三次课
